{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/Notebooks4Historical_Newspapers/blob/main/Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3OGmVNujAoi"
      },
      "source": [
        "# Researching German Historical Newspapers using the Gemini Pro Model\n",
        "## Example: Article Extraction\n",
        "\n",
        "*Notebook created by Sarah Oberbichler (oberbichler@ieg-mainz.de)*\n",
        "\n",
        "This notebook shows how LLMs can be used to support research with historical newspapers. In this example, the Gemini pro model is used to extract articles on earthquakes in OCR'd historical newspapers pages.\n",
        "\n",
        "Article segmentation for historical newspapers can be based on layout information and graphical elements (image) as well as on textual context (data). While the former is very challenging due to the changing and complex layouts of historical newspapers, the latter seems to be especially promising for topic-specific corpus building. Qualitative research relies on correctly separated articles. An article, in this context, is defined as a coherent text covering a specific topic, no more and no less.\n",
        "\n",
        "\n",
        "\n",
        "### 1.   Query the German Historical Newspaper Portal\n",
        "\n",
        "German historical newspapers from the German Digital Library can be accessed via the DDB-API. This API is open access and allows to query the Historical Newspapers available in the German Newspaper Portal ([Deutsches Zeitungsportal](https://https://www.deutsche-digitale-bibliothek.de/newspaper)). An instruction, provided by the German Newspaper Portal (from Karl Krägerlin), can be found [here](https://https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH2v8nLmn3xc"
      },
      "source": [
        "Python > 3.8 is required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebi8ay_GaFe7"
      },
      "outputs": [],
      "source": [
        "# @markdown ####  Launch this cell and get access to the API of the Newspaper Portal from the German Digital Library\n",
        "!pip install ddbapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcpFILesg9IO"
      },
      "outputs": [],
      "source": [
        "# @markdown ####  Import the necessary packages\n",
        "import pandas as pd\n",
        "from ddbapi import zp_issues, zp_pages, list_column, filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K58wQYLShHt_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @markdown ### Possible kwargs for the functions are:\n",
        "# @markdown - language: Use ISO Codes, currently ger, eng, fre, spa\n",
        "# @markdown - place_of_distribution: Search inside \"Verbreitungsort\"\n",
        "# @markdown - use a list for multiple search-words\n",
        "# @markdown - publication_date: Get newspapers by publication date.\n",
        "# @markdown - zdb_id: Search by ZDB-ID\n",
        "# @markdown - provider: Search by Data Provider\n",
        "# @markdown - paper_title: Search inside the title of the Newspaper\n",
        "# @markdown - plainpagefulltex: search inside the OCR\n",
        "# Get the data\n",
        "df = zp_pages(\n",
        "    publication_date='[1909-01-01T12:00:00Z TO 1912-01-01T12:00:00Z]',\n",
        "    plainpagefulltext=[\"Erdstoß\"],\n",
        "    paper_title='Kölnische Zeitung'\n",
        "    )\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Save the results as Excel file\n",
        "df.to_excel('name.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "CO2GMjyoQqE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### We can narrow down the text surrounding the keyword in order to reduce the input tokens for the model. Choose the size of the context window here:\n",
        "\n",
        "context_window = 5000 # @param {type:\"number\"}\n",
        "def extract_context(keyword, text, window_size=context_window):\n",
        "    index = text.find(keyword)\n",
        "    if index == -1:\n",
        "        return \"Keyword not found in text.\"\n",
        "\n",
        "    start_index = max(0, index - window_size)\n",
        "    end_index = min(len(text), index + len(keyword) + window_size)\n",
        "\n",
        "    context = text[start_index:end_index]\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "# Extract context for each row\n",
        "contexts = []\n",
        "for index, row in df.iterrows():\n",
        "    text = row['plainpagefulltext']\n",
        "    keyword = \"rdbeben\"  # You can modify this\n",
        "    context = extract_context(keyword, text)\n",
        "    contexts.append(context)\n",
        "\n",
        "# Add the context to the dataframe\n",
        "df['context'] = contexts\n",
        "\n",
        "# Print the dataframe with context\n",
        "df.head()"
      ],
      "metadata": {
        "id": "X27-D3T_i0Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Save the results as Excel file\n",
        "df.to_excel('name.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "TXUPXdj_Y3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the requirements for the Gemini model\n",
        "\n",
        "Gemini is a family of generative AI models that helps generate content and solve problems. These models are designed and trained to handle both text and images as input."
      ],
      "metadata": {
        "id": "lRGPKoOg1acb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9cmP1yv6QAZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFoMzp1l6gom"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tg0aix66nqb"
      },
      "outputs": [],
      "source": [
        "# @markdown ##### Get an API key at https://aistudio.google.com/app/prompts/new_chat, activate the pay as you go mode, and add the key to the secrets in this colab notebook (right bar). Name the key in secrets GOOGLE_API_KEY and add the key under value.\n",
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_1DyeiF7V92"
      },
      "outputs": [],
      "source": [
        "# @markdown ##### Set up the model. Find a list of the available GEMINI models here: https://ai.google.dev/gemini-api/docs/models/gemini. The safety settings can be blocked in order to have no restrictions with your data.\n",
        "model = genai.GenerativeModel('gemini-1.5-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Articles\n",
        "\n",
        "To extract articles on earthquakes, it is essential to formulate a precise prompt that specifies the articles should be extracted in their original form without translations or corrections. A guide on how to write effective prompts can be found also [here](https://https://support.google.com/a/users/answer/14200040?hl=en).\n",
        "\n",
        "Depending on the size of the dataframe, it can take a while to load."
      ],
      "metadata": {
        "id": "Iv2tH9SFeRBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[:20]"
      ],
      "metadata": {
        "id": "qMf2P2IUCBIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cafexN6GCiWd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def separate_articles(newspaper_page):\n",
        "    # Define the prompt for separating articles\n",
        "\n",
        "    response = model.generate_content(\n",
        "    [f\"Bitte separiere nur Berichte zu Erdbeben in ihrer ungeänderten\\\n",
        "      deutschen Originalform, keine Änderungen, Zusätze oder Zusammenfassungen\\\n",
        "      \\n\\n{newspaper_page}\\n\\n---\\n\\ .\"])\n",
        "    articles=response.text\n",
        "    return articles\n",
        "\n",
        "# Create an empty list to store the separated articles\n",
        "separated_articles = []\n",
        "\n",
        "# Loop through each row in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the text of the newspaper page from the current row\n",
        "    newspaper_page = row['context']\n",
        "\n",
        "    # Separate articles for the current newspaper page\n",
        "    articles = separate_articles(newspaper_page)\n",
        "\n",
        "    # Append the articles to the list\n",
        "    separated_articles.append(articles)\n",
        "\n",
        "# Add the list of separated articles as a new column 'article' in the dataframe\n",
        "df['article'] = separated_articles\n",
        "\n",
        "# Print the modified dataframe\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw9_CWK7Ftvq"
      },
      "outputs": [],
      "source": [
        "df.to_excel('name_2.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}